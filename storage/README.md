# M√≥dulo Storage - Almacenamiento Distribuido

## üìã Descripci√≥n

Sistema de almacenamiento para documentos e √≠ndices invertidos en DistriSearch.

## üìÅ Estructura

```
storage/
‚îú‚îÄ‚îÄ __init__.py           # Exports p√∫blicos
‚îú‚îÄ‚îÄ document.py           # Documentos y DocumentStore
‚îú‚îÄ‚îÄ inverted_index.py     # √çndice invertido con TF-IDF
‚îú‚îÄ‚îÄ tokenizer.py          # Tokenizaci√≥n y stopwords
‚îî‚îÄ‚îÄ persistence.py        # Persistencia en disco
```

## üéØ Componentes

### 1. `document.py`
**Gesti√≥n de documentos:**
- `Document`: Dataclass con doc_id, content, metadata
- `DocumentStore`: Almac√©n local de documentos
  - `add()`, `get()`, `update()`, `delete()`
  - Serializaci√≥n a/desde JSON

### 2. `inverted_index.py`
**√çndice invertido:**
- Estructura: `t√©rmino ‚Üí {doc_ids}`
- B√∫squeda por t√©rminos (`search()`)
- B√∫squeda AND (`search_all()`)
- **Ranking TF-IDF**:
  - `compute_tf_idf()`: Score de relevancia
  - `rank_documents()`: Ordenar por relevancia

### 3. `tokenizer.py`
**Procesamiento de texto:**
- `tokenize()`: Texto ‚Üí tokens (palabras)
- `remove_stopwords()`: Filtrado de stopwords (ES + EN)
- `compute_term_frequency()`: Frecuencia de t√©rminos

### 4. `persistence.py`
**Persistencia:**
- `PersistenceManager`: Guardar/cargar JSON
- `save_json()`, `load_json()`
- `snapshot()`: Crear snapshots
- `list_files()`, `clear_directory()`

## üîß Uso

### Almacenar Documentos

```python
from storage import Document, DocumentStore

store = DocumentStore()

doc = Document(
    doc_id="doc1",
    content="Distributed systems are complex",
    metadata={"author": "Alice"}
)

store.add(doc)
print(f"Total docs: {store.count()}")
```

### √çndice Invertido

```python
from storage import InvertedIndex, tokenize_and_filter

index = InvertedIndex()

# A√±adir documento al √≠ndice
content = "Raft consensus algorithm"
terms = tokenize_and_filter(content)
index.add_document("doc1", terms)

# Buscar
query_terms = tokenize_and_filter("consensus algorithm")
doc_ids = index.search(query_terms)

# Ranking TF-IDF
ranked = index.rank_documents(doc_ids, query_terms)
for doc_id, score in ranked:
    print(f"{doc_id}: {score:.2f}")
```

### Tokenizaci√≥n

```python
from storage.tokenizer import tokenize, remove_stopwords

text = "The distributed system uses Raft consensus"
tokens = tokenize(text)
# ['the', 'distributed', 'system', 'uses', 'raft', 'consensus']

filtered = remove_stopwords(tokens)
# ['distributed', 'system', 'uses', 'raft', 'consensus']
```

### Persistencia

```python
from storage.persistence import PersistenceManager

pm = PersistenceManager("data/node_0")

# Guardar documentos
pm.save_json("documents.json", store.to_dict())

# Guardar √≠ndice
pm.save_json("index.json", index.to_dict())

# Cargar
docs_data = pm.load_json("documents.json")
store.from_dict(docs_data)

# Snapshot
pm.snapshot("backup_2024", {
    "documents": store.to_dict(),
    "index": index.to_dict()
})
```

## üìä TF-IDF Ranking

**Term Frequency (TF)**: Frecuencia del t√©rmino en el documento

**Inverse Document Frequency (IDF)**:
```
IDF = log(N / DF)
```
donde:
- N = total de documentos
- DF = documentos que contienen el t√©rmino

**TF-IDF Score**:
```
Score = TF * IDF
```

Documentos con mayor score son m√°s relevantes.

## üéõÔ∏è Stopwords

El m√≥dulo incluye **150+ stopwords** en espa√±ol e ingl√©s:
- ES: "el", "la", "de", "que", "y", ...
- EN: "the", "a", "an", "is", "are", ...

Se pueden especificar stopwords personalizados:
```python
custom_stops = {"custom", "stop", "word"}
tokens = remove_stopwords(tokens, custom_stops)
```

## üìà Optimizaciones

1. **√çndice eficiente**: Sets para O(1) lookup
2. **Lazy evaluation**: Solo calcula TF-IDF cuando se rankea
3. **Persistencia incremental**: Solo guarda cambios
4. **Tokenizaci√≥n regex**: R√°pida y eficiente

## ‚úÖ Serializaci√≥n

Todos los componentes son **serializables a JSON**:
```python
# DocumentStore
data = store.to_dict()
store.from_dict(data)

# InvertedIndex
data = index.to_dict()
index.from_dict(data)
```

## üöÄ Integraci√≥n

```python
from storage import (
    Document, DocumentStore,
    InvertedIndex,
    tokenize_and_filter,
    PersistenceManager
)

# Setup
store = DocumentStore()
index = InvertedIndex()
persistence = PersistenceManager("data/node_0")

# A√±adir documento
doc = Document("doc1", "Distributed consensus")
store.add(doc)

terms = tokenize_and_filter(doc.content)
index.add_document(doc.doc_id, terms)

# Guardar
persistence.save_json("documents.json", store.to_dict())
persistence.save_json("index.json", index.to_dict())
```
