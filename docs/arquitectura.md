# Arquitectura del Sistema - DistriSearch

Esta secciÃ³n describe la arquitectura tÃ©cnica de DistriSearch basada en el modelo **Master-Slave** con ubicaciÃ³n semÃ¡ntica de recursos.

---

## ğŸ—ï¸ Arquitectura General: Master-Slave

DistriSearch utiliza una arquitectura **Master-Slave distribuida** donde:

- **Cualquier nodo puede ser Master** (elecciÃ³n dinÃ¡mica mediante algoritmo Bully)
- **Todos los nodos son Slaves** por defecto
- **El Master coordina** bÃºsquedas, replicaciÃ³n y ubicaciÃ³n de recursos
- **Los Slaves almacenan** documentos y responden queries

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DistriSearch Cluster                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚    â”‚   CoreDNS    â”‚  â† ResoluciÃ³n DNS con failover              â”‚
â”‚    â”‚  (DNS Round  â”‚                                             â”‚
â”‚    â”‚   Robin)     â”‚                                             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚           â”‚                                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚    â”‚                                                â”‚            â”‚
â”‚    â–¼                    â–¼                    â–¼      â”‚            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚            â”‚
â”‚ â”‚  Node 1  â”‚      â”‚  Node 2  â”‚      â”‚  Node 3  â”‚   â”‚            â”‚
â”‚ â”‚ (MASTER) â”‚â—„â”€â”€â”€â”€â–ºâ”‚ (SLAVE)  â”‚â—„â”€â”€â”€â”€â–ºâ”‚ (SLAVE)  â”‚   â”‚            â”‚
â”‚ â”‚          â”‚      â”‚          â”‚      â”‚          â”‚   â”‚            â”‚
â”‚ â”‚ Backend  â”‚      â”‚ Backend  â”‚      â”‚ Backend  â”‚   â”‚            â”‚
â”‚ â”‚ Frontend â”‚      â”‚ Frontend â”‚      â”‚ Frontend â”‚   â”‚            â”‚
â”‚ â”‚ MongoDB  â”‚      â”‚ MongoDB  â”‚      â”‚ MongoDB  â”‚   â”‚            â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚            â”‚
â”‚       â”‚                â”‚                â”‚           â”‚            â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚            â”‚
â”‚                        â”‚                            â”‚            â”‚
â”‚              Heartbeats UDP (puerto 5000)           â”‚            â”‚
â”‚              ElecciÃ³n Bully (puerto 5001)           â”‚            â”‚
â”‚                                                     â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Componentes Principales

### 1. Core (CÃ³digo Compartido)

MÃ³dulos compartidos entre Master y Slaves:

```
core/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py      # ClusterConfig - ConfiguraciÃ³n del cluster
â””â”€â”€ models.py      # NodeInfo, ClusterMessage, SlaveProfile, etc.
```

**ConfiguraciÃ³n del Nodo** (`core/config.py`):

| ParÃ¡metro | Tipo | DescripciÃ³n |
|-----------|------|-------------|
| `node_id` | string | ID Ãºnico del nodo |
| `node_role` | enum | "master" o "slave" |
| `master_candidate` | bool | Â¿Puede ser elegido Master? |
| `heartbeat_interval` | int | Segundos entre heartbeats |
| `heartbeat_timeout` | int | Timeout para considerar nodo caÃ­do |
| `replication_factor` | int | NÃºmero de rÃ©plicas (K) |
| `embedding_model` | string | Modelo para embeddings semÃ¡nticos |

### 2. Master (LÃ³gica de CoordinaciÃ³n)

El Master coordina el cluster:

```
master/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ embedding_service.py       # GeneraciÃ³n de embeddings semÃ¡nticos
â”œâ”€â”€ location_index.py          # Ãndice de ubicaciÃ³n de documentos
â”œâ”€â”€ load_balancer.py           # Balanceo de carga entre Slaves
â”œâ”€â”€ query_router.py            # Enrutamiento de bÃºsquedas
â””â”€â”€ replication_coordinator.py # CoordinaciÃ³n de replicaciÃ³n
```

**Funcionalidades del Master**:

| Componente | Responsabilidad |
|------------|-----------------|
| `EmbeddingService` | Genera vectores semÃ¡nticos de documentos/queries usando `sentence-transformers` |
| `SemanticLocationIndex` | Ãndice de ubicaciÃ³n por similitud semÃ¡ntica |
| `LoadBalancer` | Distribuye carga segÃºn afinidad y estado (weighted, round-robin, least-connections) |
| `QueryRouter` | Enruta queries a Slaves relevantes |
| `ReplicationCoordinator` | Coordina rÃ©plicas por afinidad semÃ¡ntica |

### 3. Backend (API y Servicios)

Cada nodo ejecuta un backend FastAPI:

```
backend/
â”œâ”€â”€ main.py                 # Punto de entrada
â”œâ”€â”€ database.py             # ConexiÃ³n MongoDB
â”œâ”€â”€ models.py               # Modelos Pydantic
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ auth.py            # AutenticaciÃ³n JWT
â”‚   â”œâ”€â”€ search.py          # BÃºsqueda distribuida
â”‚   â”œâ”€â”€ register.py        # Registro de nodos y archivos
â”‚   â”œâ”€â”€ download.py        # Descarga de archivos
â”‚   â”œâ”€â”€ cluster.py         # Operaciones de cluster
â”‚   â””â”€â”€ health.py          # Health checks
â””â”€â”€ services/
    â”œâ”€â”€ heartbeat.py       # Sistema de heartbeats UDP
    â”œâ”€â”€ election.py        # Algoritmo Bully para elecciÃ³n
    â”œâ”€â”€ node_service.py    # GestiÃ³n de nodos
    â”œâ”€â”€ replication_service.py
    â”œâ”€â”€ dynamic_replication.py
    â””â”€â”€ reliability_metrics.py  # MTTR/MTBF
```

### 4. Frontend (Streamlit)

Interfaz web por nodo:

```
frontend/
â”œâ”€â”€ app.py                 # Home con autenticaciÃ³n
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 01_ğŸ”_Buscar.py   # BÃºsqueda distribuida
â”‚   â”œâ”€â”€ 02_ğŸŒ_Nodos.py    # GestiÃ³n de nodos
â”‚   â”œâ”€â”€ 03_ğŸ“Š_EstadÃ­sticas.py
â”‚   â””â”€â”€ 04_ğŸ“¤_Subir_Archivos.py
â””â”€â”€ utils/
    â””â”€â”€ api_client.py      # Cliente HTTP
```

### 5. DNS (CoreDNS)

ResoluciÃ³n DNS con failover automÃ¡tico:

```
dns/
â”œâ”€â”€ Corefile    # ConfiguraciÃ³n CoreDNS
â””â”€â”€ hosts       # Hosts dinÃ¡micos (se actualizan automÃ¡ticamente)
```

---

## ğŸ”„ Flujos de Datos

### Flujo de BÃºsqueda Distribuida

1. Usuario ingresa query en Frontend
2. Frontend envÃ­a `POST /search` al Backend local
3. Backend (si es Master o conoce al Master):
   - Genera embedding de la query
   - Identifica Slaves con contenido similar (ubicaciÃ³n semÃ¡ntica)
   - EnvÃ­a query en paralelo a Slaves relevantes
4. Slaves buscan en su MongoDB local
5. Master agrega y rankea resultados
6. Resultados se devuelven al Frontend

### Flujo de ElecciÃ³n de LÃ­der (Bully Algorithm)

```
1. Node_1 detecta que Master no responde (3 heartbeats fallidos)

2. Node_1 inicia elecciÃ³n:
   Node_1 â”€â”€â”€â”€ELECTIONâ”€â”€â”€â”€â–º Node_2 (ID mayor)
   Node_1 â”€â”€â”€â”€ELECTIONâ”€â”€â”€â”€â–º Node_3 (ID mayor)

3. Nodos con ID mayor responden:
   Node_2 â”€â”€â”€â”€ELECTION_OKâ”€â”€â–º Node_1
   Node_3 â”€â”€â”€â”€ELECTION_OKâ”€â”€â–º Node_1

4. Node_1 espera... Node_3 (mayor ID) debe proclamarse

5. Node_3 gana y se proclama:
   Node_3 â”€â”€â”€â”€COORDINATORâ”€â”€â–º Node_1
   Node_3 â”€â”€â”€â”€COORDINATORâ”€â”€â–º Node_2

6. Todos reconocen a Node_3 como nuevo Master
```

### Flujo de ReplicaciÃ³n por Afinidad SemÃ¡ntica

1. Usuario sube documento a Node_1
2. Node_1 notifica al Master
3. Master genera embedding del documento
4. Master selecciona Slaves con contenido semÃ¡nticamente similar
5. Master coordina replicaciÃ³n a nodos seleccionados
6. Se mantiene factor de replicaciÃ³n K=2

---

## ğŸŒ TopologÃ­a de Red

### ConfiguraciÃ³n Docker

```yaml
networks:
  distrisearch_cluster:
    subnet: 172.20.0.0/24

# IPs Fijas:
# DNS:     172.20.0.2
# Node_1:  172.20.0.11 (backend), 172.20.0.12 (frontend)
# Node_2:  172.20.0.21 (backend), 172.20.0.22 (frontend)
# Node_3:  172.20.0.31 (backend), 172.20.0.32 (frontend)
```

### Puertos

| Puerto | Protocolo | Uso |
|--------|-----------|-----|
| 8000 | HTTP | API Backend |
| 8443 | HTTPS | API Backend (SSL) |
| 8501 | HTTP | Frontend Streamlit |
| 5000 | UDP | Heartbeats |
| 5001 | UDP | ElecciÃ³n de lÃ­der |
| 27017 | TCP | MongoDB |
| 53 | UDP/TCP | DNS |

---

## ğŸ›¡ï¸ Tolerancia a Fallos

### Sistema de Heartbeats

- **Protocolo**: UDP
- **Intervalo**: 5 segundos
- **Timeout**: 15 segundos (3 beats fallidos)
- **AcciÃ³n**: Marcar nodo como `offline`, iniciar recuperaciÃ³n de rÃ©plicas

### ElecciÃ³n de LÃ­der

- **Algoritmo**: Bully
- **Trigger**: Master no responde a 3 heartbeats consecutivos
- **Criterio**: Gana el nodo con mayor `node_id` (lexicogrÃ¡fico)
- **Tiempo de elecciÃ³n**: ~10-15 segundos

### ReplicaciÃ³n

- **Factor por defecto**: K=2
- **Criterio de selecciÃ³n**: Afinidad semÃ¡ntica (nodos con contenido similar)
- **Modelo de consistencia**: Eventual (Last-Write-Wins)
- **RecuperaciÃ³n**: AutomÃ¡tica ante fallo de Slave

---

## ğŸ“Š MÃ©tricas de Confiabilidad

El sistema registra automÃ¡ticamente:

- **MTTR** (Mean Time To Recovery): Tiempo promedio de recuperaciÃ³n
- **MTBF** (Mean Time Between Failures): Tiempo entre fallos
- **Disponibilidad**: `MTBF / (MTBF + MTTR)`

Endpoint: `GET /health/cluster`

---

## ğŸ”§ ConfiguraciÃ³n por Variables de Entorno

```bash
# IdentificaciÃ³n
NODE_ID=node_1
NODE_ROLE=slave
MASTER_CANDIDATE=true

# Red
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
EXTERNAL_IP=172.20.0.11

# Cluster
CLUSTER_PEERS=node_2:172.20.0.21:8000:5000:5001,node_3:172.20.0.31:8000:5000:5001
HEARTBEAT_INTERVAL=5
HEARTBEAT_TIMEOUT=15

# ReplicaciÃ³n
REPLICATION_FACTOR=2
CONSISTENCY_MODEL=eventual

# Base de datos
MONGO_URI=mongodb://localhost:27017
MONGO_DBNAME=distrisearch

# Embeddings (ubicaciÃ³n semÃ¡ntica)
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

---

## ğŸš€ Despliegue

### Docker Compose (Cluster de 3 nodos)

```bash
cd DistriSearch/deploy
docker-compose -f docker-compose.cluster.yml up -d
```

Esto levanta:
- 1 servidor DNS (CoreDNS)
- 3 nodos completos (backend + frontend + MongoDB cada uno)

### URLs de Acceso

| Componente | URL |
|------------|-----|
| Frontend Node 1 | http://localhost:8511 |
| Frontend Node 2 | http://localhost:8512 |
| Frontend Node 3 | http://localhost:8513 |
| API Node 1 | http://localhost:8001 |
| API Node 2 | http://localhost:8002 |
| API Node 3 | http://localhost:8003 |

---

## ğŸ”Œ API Endpoints Principales

### Health Checks

| Endpoint | DescripciÃ³n |
|----------|-------------|
| `GET /health` | Check bÃ¡sico |
| `GET /health/detailed` | MÃ©tricas del sistema |
| `GET /health/cluster` | Estado del cluster |
| `GET /health/ready` | Readiness probe |
| `GET /health/live` | Liveness probe |

### BÃºsqueda

| Endpoint | DescripciÃ³n |
|----------|-------------|
| `GET /search/?q={query}` | BÃºsqueda distribuida |
| `GET /search/nodes` | Lista de nodos |

### Registro

| Endpoint | DescripciÃ³n |
|----------|-------------|
| `POST /register/node` | Registrar nodo |
| `POST /register/files` | Registrar archivos |
| `POST /register/upload` | Subir archivo |

### Cluster

| Endpoint | DescripciÃ³n |
|----------|-------------|
| `GET /cluster/status` | Estado del cluster |
| `POST /cluster/election` | Forzar elecciÃ³n |

---

[:octicons-arrow-left-24: Volver](index.md){ .md-button }
[:octicons-arrow-right-24: CaracterÃ­sticas](caracteristicas.md){ .md-button .md-button--primary }
