# ğŸ“ Arquitectura del Sistema

<div style="padding: 1.5rem; background: linear-gradient(135deg, rgba(102, 126, 234, 0.1) 0%, rgba(118, 75, 162, 0.1) 100%); border-radius: 16px; margin-bottom: 2rem;">
  <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.5rem;">
    <span style="background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 600;">v2.0</span>
    <span style="background: #10b981; color: white; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 600;">Master-Slave</span>
    <span style="background: #f59e0b; color: white; padding: 0.2rem 0.6rem; border-radius: 12px; font-size: 0.75rem; font-weight: 600;">Bully Election</span>
  </div>
  <p style="margin: 0.5rem 0 0 0; color: #718096;">DocumentaciÃ³n tÃ©cnica de la arquitectura <strong>Master-Slave distribuida</strong> con ubicaciÃ³n semÃ¡ntica de recursos.</p>
</div>

---

## ğŸ—ï¸ VisiÃ³n General

DistriSearch implementa una arquitectura **Master-Slave distribuida con failover automÃ¡tico**:

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin: 1.5rem 0;">

<div style="padding: 1.2rem; background: rgba(102, 126, 234, 0.08); border-radius: 12px; border: 1px solid rgba(102, 126, 234, 0.15); text-align: center;">
  <span style="font-size: 2rem;">ğŸ‘‘</span>
  <h4 style="margin: 0.5rem 0 0.3rem 0;">Cualquier Nodo = Master</h4>
  <p style="margin: 0; font-size: 0.85rem; color: #718096;">ElecciÃ³n dinÃ¡mica con Bully</p>
</div>

<div style="padding: 1.2rem; background: rgba(16, 185, 129, 0.08); border-radius: 12px; border: 1px solid rgba(16, 185, 129, 0.15); text-align: center;">
  <span style="font-size: 2rem;">ğŸ–¥ï¸</span>
  <h4 style="margin: 0.5rem 0 0.3rem 0;">Todos son Slaves</h4>
  <p style="margin: 0; font-size: 0.85rem; color: #718096;">Nodos autÃ³nomos por defecto</p>
</div>

<div style="padding: 1.2rem; background: rgba(245, 158, 11, 0.08); border-radius: 12px; border: 1px solid rgba(245, 158, 11, 0.15); text-align: center;">
  <span style="font-size: 2rem;">ğŸ§ </span>
  <h4 style="margin: 0.5rem 0 0.3rem 0;">Master Coordina</h4>
  <p style="margin: 0; font-size: 0.85rem; color: #718096;">BÃºsquedas y replicaciÃ³n</p>
</div>

<div style="padding: 1.2rem; background: rgba(239, 68, 68, 0.08); border-radius: 12px; border: 1px solid rgba(239, 68, 68, 0.15); text-align: center;">
  <span style="font-size: 2rem;">ğŸ›¡ï¸</span>
  <h4 style="margin: 0.5rem 0 0.3rem 0;">Sin Punto de Fallo</h4>
  <p style="margin: 0; font-size: 0.85rem; color: #718096;">Failover automÃ¡tico ~15s</p>
</div>

</div>

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#667eea', 'primaryTextColor': '#fff', 'primaryBorderColor': '#5a67d8', 'lineColor': '#a0aec0', 'secondaryColor': '#764ba2', 'tertiaryColor': '#10b981'}}}%%
flowchart TB
    subgraph CLUSTER["ğŸŒ DistriSearch Cluster"]
        direction TB
        
        DNS["<b>ğŸŒ CoreDNS</b><br/>distrisearch.local<br/>Round-Robin + Failover"]
        
        subgraph NODE1["<b>ğŸ“¦ Node 1</b> (MASTER)"]
            direction LR
            M_API["ğŸ”§ API<br/>:8001"]
            M_UI["ğŸ¨ UI<br/>:8511"]
            M_DB[("ğŸ—„ï¸ MongoDB")]
            M_IDX["ğŸ§  Ãndice<br/>SemÃ¡ntico"]
        end
        
        subgraph NODE2["<b>ğŸ“¦ Node 2</b> (SLAVE)"]
            direction LR
            S1_API["ğŸ”§ API<br/>:8002"]
            S1_UI["ğŸ¨ UI<br/>:8512"]
            S1_DB[("ğŸ—„ï¸ MongoDB")]
        end
        
        subgraph NODE3["<b>ğŸ“¦ Node 3</b> (SLAVE)"]
            direction LR
            S2_API["ğŸ”§ API<br/>:8003"]
            S2_UI["ğŸ¨ UI<br/>:8513"]
            S2_DB[("ğŸ—„ï¸ MongoDB")]
        end
    end
    
    CLIENT(["ğŸ‘¤ Cliente"]) --> DNS
    DNS --> M_API & S1_API & S2_API
    
    M_API <-.->|"ğŸ’“ UDP :5000"| S1_API
    M_API <-.->|"ğŸ’“ UDP :5000"| S2_API
    S1_API <-.->|"ğŸ’“ UDP :5000"| S2_API
    
    M_API -->|"Coordina"| S1_API & S2_API
    
    style DNS fill:#10b981,stroke:#059669,color:#fff
    style M_API fill:#667eea,stroke:#5a67d8,color:#fff
    style M_IDX fill:#f59e0b,stroke:#d97706,color:#fff
    style S1_API fill:#764ba2,stroke:#6b46c1,color:#fff
    style S2_API fill:#764ba2,stroke:#6b46c1,color:#fff
    style CLIENT fill:#3b82f6,stroke:#2563eb,color:#fff
```

### MÃ©tricas Clave

| CaracterÃ­stica | Valor | DescripciÃ³n |
|:---------------|:------|:------------|
| **Latencia de consulta** | $O(1)$ saltos | Master enruta directamente al Slave apropiado |
| **Escalabilidad** | Lineal | AÃ±adir mÃ¡s Slaves aumenta capacidad |
| **Tolerancia a fallos** | Alta | Sistema opera si falla cualquier nodo |
| **Tiempo de elecciÃ³n** | ~10-15 s | Algoritmo Bully |
| **Factor de replicaciÃ³n** | K=2 | Configurable |

---

## ğŸ“¦ Estructura del Proyecto

La arquitectura del cÃ³digo sigue una separaciÃ³n clara de responsabilidades:

```
DistriSearch/
â”œâ”€â”€ core/                          # ğŸ”§ CÃ³digo compartido por todos los nodos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n unificada del cluster
â”‚   â””â”€â”€ models.py                 # Modelos de datos: NodeInfo, MessageType, etc.
â”‚
â”œâ”€â”€ cluster/                       # ğŸ”— Servicios de coordinaciÃ³n del cluster
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ heartbeat.py              # Sistema de heartbeats UDP
â”‚   â”œâ”€â”€ election.py               # Algoritmo Bully para elecciÃ³n de lÃ­der
â”‚   â”œâ”€â”€ discovery.py              # Descubrimiento de nodos (multicast)
â”‚   â””â”€â”€ naming/                   # Sistema de nombres
â”‚       â”œâ”€â”€ hierarchical.py       # Naming jerÃ¡rquico
â”‚       â””â”€â”€ ip_cache.py           # CachÃ© de IPs
â”‚
â”œâ”€â”€ master/                        # ğŸ‘‘ LÃ³gica especÃ­fica del rol Master
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedding_service.py      # GeneraciÃ³n de embeddings semÃ¡nticos
â”‚   â”œâ”€â”€ location_index.py         # Ãndice de ubicaciÃ³n semÃ¡ntica
â”‚   â”œâ”€â”€ load_balancer.py          # Balanceo de carga entre Slaves
â”‚   â”œâ”€â”€ query_router.py           # Enrutamiento de bÃºsquedas
â”‚   â””â”€â”€ replication_coordinator.py # CoordinaciÃ³n de replicaciÃ³n
â”‚
â”œâ”€â”€ slave/                         # ğŸ–¥ï¸ LÃ³gica del nodo Slave
â”‚   â”œâ”€â”€ api/                      # API REST (FastAPI)
â”‚   â”‚   â”œâ”€â”€ main.py               # Punto de entrada
â”‚   â”‚   â”œâ”€â”€ database.py           # ConexiÃ³n MongoDB
â”‚   â”‚   â”œâ”€â”€ models.py             # Modelos Pydantic
â”‚   â”‚   â”œâ”€â”€ auth.py               # AutenticaciÃ³n JWT
â”‚   â”‚   â””â”€â”€ routes/               # Endpoints
â”‚   â”‚       â”œâ”€â”€ search.py         # BÃºsqueda distribuida
â”‚   â”‚       â”œâ”€â”€ register.py       # Registro de nodos/archivos
â”‚   â”‚       â”œâ”€â”€ download.py       # Descarga de archivos
â”‚   â”‚       â”œâ”€â”€ cluster.py        # Operaciones de cluster
â”‚   â”‚       â””â”€â”€ health.py         # Health checks
â”‚   â””â”€â”€ services/                 # Servicios del Slave
â”‚       â”œâ”€â”€ index_service.py
â”‚       â”œâ”€â”€ node_service.py
â”‚       â””â”€â”€ replication_service.py
â”‚
â”œâ”€â”€ frontend/                      # ğŸ¨ UI Streamlit
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ pages/
â”‚
â”œâ”€â”€ deploy/                        # ğŸš€ ConfiguraciÃ³n de despliegue
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ docker-compose.cluster.yml
â”‚
â”œâ”€â”€ dns/                           # ğŸŒ CoreDNS
â”‚   â”œâ”€â”€ Corefile
â”‚   â””â”€â”€ hosts
â”‚
â””â”€â”€ tests/                         # âœ… Tests
    â”œâ”€â”€ unit/
    â”œâ”€â”€ integration/
    â””â”€â”€ e2e/
```

---

## ğŸ”§ Componentes Detallados

### 1. Core - CÃ³digo Compartido

MÃ³dulos compartidos entre Master y Slaves que definen la base del sistema:

#### ConfiguraciÃ³n (`core/config.py`)

| ParÃ¡metro | Tipo | Default | DescripciÃ³n |
|-----------|------|---------|-------------|
| `node_id` | string | `node_1` | Identificador Ãºnico del nodo |
| `node_role` | enum | `slave` | Rol inicial: "master" o "slave" |
| `master_candidate` | bool | `true` | Â¿Puede ser elegido Master? |
| `heartbeat_interval` | int | `5` | Segundos entre heartbeats |
| `heartbeat_timeout` | int | `15` | Timeout para considerar nodo caÃ­do |
| `replication_factor` | int | `2` | NÃºmero de rÃ©plicas (K) |
| `embedding_model` | string | `all-MiniLM-L6-v2` | Modelo para embeddings (384 dims) |
| `mongo_uri` | string | `mongodb://localhost:27017` | URI de MongoDB |

#### Modelos (`core/models.py`)

```python
# Enums
class NodeRole(Enum): SLAVE, MASTER
class NodeStatus(Enum): ONLINE, OFFLINE, UNKNOWN, STARTING
class MessageType(Enum): 
    PING, PONG,                    # Heartbeat
    ELECTION, ELECTION_OK, COORDINATOR,  # Bully
    REGISTER_CONTENT, QUERY_ROUTING, REPLICATE  # Datos

# Dataclasses principales
@dataclass
class NodeInfo:
    node_id: str
    ip_address: str
    port: int
    status: NodeStatus
    is_master: bool
    can_be_master: bool
    document_count: int
    last_seen: datetime

@dataclass
class ClusterMessage:
    type: MessageType
    sender_id: str
    payload: Dict
    timestamp: datetime

@dataclass
class SlaveProfile:
    slave_id: str
    embedding: np.ndarray  # Perfil semÃ¡ntico agregado
    document_count: int
    load_score: float      # 0.0 = sin carga, 1.0 = mÃ¡xima
```

---

### 2. Cluster - Servicios de CoordinaciÃ³n

#### Sistema de Heartbeats (`cluster/heartbeat.py`)

Monitoreo de nodos mediante UDP para detecciÃ³n de fallos:

- **Protocolo**: UDP (puerto 5000)
- **Intervalo**: 5 segundos
- **Timeout**: 15 segundos (3 beats fallidos)
- **Mensajes**: `PING` â†’ `PONG`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          PING          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node A â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Node B â”‚
â”‚         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          PONG          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Algoritmo Bully (`cluster/election.py`)

ElecciÃ³n de lÃ­der cuando el Master falla:

```
Algoritmo Bully para elecciÃ³n de Master:

1. Node_i detecta que Master no responde
2. Node_i envÃ­a ELECTION a todos Node_j donde j > i
3. Si ningÃºn Node_j responde OK en timeout:
   â†’ Node_i se convierte en Master
   â†’ Node_i envÃ­a COORDINATOR a todos
4. Si algÃºn Node_j responde OK:
   â†’ Node_i espera mensaje COORDINATOR

Ejemplo con IDs 50, 60, 70 (Master=100 falla):
  Slave-50 â”€â”€ELECTIONâ”€â”€â–º 60, 70
  Slave-60 â”€â”€OKâ”€â”€â–º 50
  Slave-70 â”€â”€OKâ”€â”€â–º 50
  Slave-60 â”€â”€ELECTIONâ”€â”€â–º 70
  Slave-70 â”€â”€OKâ”€â”€â–º 60
  Slave-70 â”€â”€COORDINATORâ”€â”€â–º todos (nuevo Master)
```

---

### 3. Master - LÃ³gica de CoordinaciÃ³n

El Master es un Slave que asume responsabilidades adicionales:

| Componente | Responsabilidad |
|------------|-----------------|
| **EmbeddingService** | Genera vectores semÃ¡nticos (384 dims) usando `sentence-transformers` |
| **SemanticLocationIndex** | Ãndice de ubicaciÃ³n por similitud coseno |
| **LoadBalancer** | Distribuye carga segÃºn afinidad y estado de nodos |
| **QueryRouter** | Enruta queries a Slaves con contenido semÃ¡nticamente relevante |
| **ReplicationCoordinator** | Selecciona nodos para rÃ©plicas por afinidad semÃ¡ntica |

#### Flujo de BÃºsqueda SemÃ¡ntica

```
1. Usuario envÃ­a query al Master
2. Master calcula embedding de la query: q_embedding
3. Master ordena Slaves por similitud: cos(q_embedding, slave_profile)
4. Master envÃ­a query a top-3 Slaves mÃ¡s relevantes
5. Slaves ejecutan bÃºsqueda local en MongoDB
6. Master agrega y rankea resultados finales
```

#### SelecciÃ³n de Nodos para ReplicaciÃ³n

```
slave_destino = argmax_{s âˆˆ slaves} cos(embedding_doc, profile_s)
```

Criterios de selecciÃ³n:
1. **Afinidad semÃ¡ntica**: Slaves con perfil similar al documento
2. **Carga actual**: Balancear distribuciÃ³n de almacenamiento
3. **Disponibilidad**: Priorizar Slaves con alto uptime

---

### 4. Slave - Nodo Trabajador

Cada Slave es un nodo autÃ³nomo con:

- **Backend API** (FastAPI): Procesa requests REST
- **Frontend** (Streamlit): Interfaz de usuario
- **MongoDB**: Almacenamiento local de documentos

#### API Endpoints

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/health` | GET | Check bÃ¡sico |
| `/health/cluster` | GET | Estado del cluster |
| `/search/?q={query}` | GET | BÃºsqueda distribuida |
| `/register/node` | POST | Registrar nodo |
| `/register/files` | POST | Registrar archivos |
| `/register/upload` | POST | Subir archivo |
| `/download/{file_id}` | GET | Descargar archivo |
| `/cluster/status` | GET | Estado del cluster |
| `/cluster/election` | POST | Forzar elecciÃ³n |

---

### 5. Frontend (Streamlit)

Interfaz web servida por cada Slave:

```
frontend/
â”œâ”€â”€ app.py                    # Home con autenticaciÃ³n
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 01_ğŸ”_Buscar.py      # BÃºsqueda distribuida
â”‚   â”œâ”€â”€ 02_ğŸŒ_Nodos.py       # GestiÃ³n de nodos
â”‚   â”œâ”€â”€ 03_ğŸ“Š_EstadÃ­sticas.py # MÃ©tricas del cluster
â”‚   â””â”€â”€ 04_ğŸ“¤_Subir_Archivos.py
â””â”€â”€ utils/
    â””â”€â”€ api_client.py         # Cliente HTTP para backend
```

---

### 6. DNS (CoreDNS)

ResoluciÃ³n DNS con failover automÃ¡tico:

- **Dominio**: `distrisearch.local`
- **ResoluciÃ³n**: Round-robin a Slaves saludables
- **Failover**: AutomÃ¡tico si un Slave no responde

```
# Corefile
distrisearch.local:53 {
    hosts /etc/coredns/hosts {
        fallthrough
    }
    forward . 8.8.8.8
}

# hosts (se actualiza dinÃ¡micamente)
172.20.0.11 distrisearch.local
172.20.0.21 distrisearch.local
172.20.0.31 distrisearch.local
```

---

## ğŸ”„ Flujos de Datos

### Flujo de Consulta TÃ­pico

```
1. Cliente envÃ­a consulta al Master (o cualquier Slave la redirige)
2. Master calcula embedding semÃ¡ntico de la consulta
3. Master identifica Slaves con contenido semÃ¡nticamente relevante
4. Master enruta la consulta a los Slaves seleccionados (top-3)
5. Slaves ejecutan bÃºsqueda local en MongoDB y devuelven resultados
6. Master agrega, rankea y devuelve resultados finales al cliente
```

### Flujo de BÃºsqueda Distribuida (Detallado)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    1. Query     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Usuario â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Frontend â”‚
â”‚          â”‚                 â”‚(Streamlit)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                  â”‚
                          2. POST /search
                                  â”‚
                                  â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Backend â”‚ (Slave local)
                            â”‚ (FastAPI)â”‚
                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                  â”‚
                   3. Si tiene info del Master
                                  â”‚
                                  â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Master  â”‚
                            â”‚          â”‚
                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                  â”‚
            4. Calcula embedding â”€â”˜
               q_emb = encode(query)
                                  â”‚
            5. Ordena Slaves â”€â”€â”€â”€â”€â”˜
               por cos(q_emb, slave_profile)
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼             â–¼             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Slave 1 â”‚   â”‚ Slave 2 â”‚   â”‚ Slave 3 â”‚
              â”‚  (top 1)â”‚   â”‚  (top 2)â”‚   â”‚  (top 3)â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                   â”‚             â”‚             â”‚
         6. BÃºsqueda local MongoDB
                   â”‚             â”‚             â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                         7. Agregar resultados
                                 â”‚
                                 â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚ Respuestaâ”‚
                           â”‚ al user  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de ElecciÃ³n de LÃ­der (Bully Algorithm)

```
Escenario: Master (node_id=100) falla, Slaves con IDs 50, 60, 70

Tiempo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

t0: Master falla (no envÃ­a heartbeats)

t1: Slave-50 detecta timeout (3 heartbeats fallidos)
    Slave-50 â”€â”€â”€â”€ELECTIONâ”€â”€â”€â”€â–º Slave-60
    Slave-50 â”€â”€â”€â”€ELECTIONâ”€â”€â”€â”€â–º Slave-70

t2: Nodos con ID mayor responden OK
    Slave-60 â”€â”€â”€â”€ELECTION_OKâ”€â”€â–º Slave-50
    Slave-70 â”€â”€â”€â”€ELECTION_OKâ”€â”€â–º Slave-50

t3: Slave-50 espera... deja que nodos mayores compitan

t4: Slave-60 inicia su propia elecciÃ³n
    Slave-60 â”€â”€â”€â”€ELECTIONâ”€â”€â”€â”€â–º Slave-70

t5: Slave-70 responde OK
    Slave-70 â”€â”€â”€â”€ELECTION_OKâ”€â”€â–º Slave-60

t6: Slave-70 no tiene nadie con ID mayor
    Slave-70 se proclama COORDINATOR

t7: Slave-70 anuncia a todos
    Slave-70 â”€â”€â”€â”€COORDINATORâ”€â”€â–º Slave-50
    Slave-70 â”€â”€â”€â”€COORDINATORâ”€â”€â–º Slave-60

t8: Todos reconocen a Slave-70 como nuevo Master âœ“
```

### Flujo de ReplicaciÃ³n por Afinidad SemÃ¡ntica

```
1. Usuario sube documento D a Slave-1
   POST /register/upload

2. Slave-1 almacena D en su MongoDB local
   â–º Genera file_id Ãºnico (UUID)
   â–º Extrae contenido textual

3. Slave-1 notifica al Master
   POST /master/register_content
   {file_id, filename, content_preview}

4. Master genera embedding del documento
   doc_emb = embedding_service.encode(content)

5. Master actualiza su Ã­ndice de ubicaciÃ³n
   location_index.register(file_id, doc_emb, slave_1)

6. Master selecciona Slaves para rÃ©plicas
   Para K=2: necesita 1 rÃ©plica adicional
   target = argmax_{s âˆˆ slaves, s â‰  slave_1} cos(doc_emb, profile_s)
   
   Ejemplo: D es sobre "Machine Learning"
   â–º Slave-2 tiene perfil "Data Science, ML" â†’ alta afinidad
   â–º Slave-3 tiene perfil "Networking" â†’ baja afinidad
   â–º Se elige Slave-2

7. Master coordina replicaciÃ³n
   replication_coordinator.replicate(file_id, slave_1, [slave_2])

8. Slave-1 transfiere D a Slave-2
   â–º ConexiÃ³n directa Slave-Slave (no pasa por Master)

9. Master actualiza Ã­ndice con nueva ubicaciÃ³n
   location_index.add_replica(file_id, slave_2)
```

---

## ğŸŒ TopologÃ­a de Red

### TopologÃ­a en Estrella con Redundancia

La red se estructura como una **topologÃ­a en estrella** donde el Master actÃºa como nodo central, pero todos los Slaves mantienen conexiones entre sÃ­ para tolerancia a fallos.

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Master  â”‚
                    â”‚(Slave-1)â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚             â”‚             â”‚
      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
      â”‚ Slave-2 â”‚â”€â”€â”€â”‚ Slave-3 â”‚â”€â”€â”€â”‚ Slave-4 â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚             â”‚             â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  (heartbeats)
    
Leyenda:
  â”€â”€â”€ ComunicaciÃ³n primaria (Master â†” Slave)
  ... ComunicaciÃ³n secundaria (Slave â†” Slave)
```

### ConfiguraciÃ³n Docker Network

```yaml
networks:
  distrisearch_cluster:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24

# IPs Fijas asignadas:
# DNS:            172.20.0.2
# Slave-1 (API):  172.20.0.11
# Slave-1 (Web):  172.20.0.12
# Slave-2 (API):  172.20.0.21
# Slave-2 (Web):  172.20.0.22
# Slave-3 (API):  172.20.0.31
# Slave-3 (Web):  172.20.0.32
```

### Puertos del Sistema

| Puerto | Protocolo | Uso | DescripciÃ³n |
|--------|-----------|-----|-------------|
| 8000 | TCP/HTTP | API Backend | Endpoints REST FastAPI |
| 8443 | TCP/HTTPS | API Backend SSL | Endpoints REST con TLS |
| 8501 | TCP/HTTP | Frontend Streamlit | Interfaz de usuario |
| 5000 | UDP | Heartbeats | DetecciÃ³n de nodos caÃ­dos |
| 5001 | UDP | ElecciÃ³n Bully | Mensajes ELECTION/COORDINATOR |
| 27017 | TCP | MongoDB | Base de datos local |
| 53 | UDP/TCP | DNS | CoreDNS para resoluciÃ³n |

---

## ğŸ›¡ï¸ Tolerancia a Fallos

### Sistema de Heartbeats

```
ConfiguraciÃ³n:
â”œâ”€â”€ Protocolo:       UDP
â”œâ”€â”€ Intervalo:       5 segundos (HEARTBEAT_INTERVAL)
â”œâ”€â”€ Timeout:         15 segundos (HEARTBEAT_TIMEOUT)
â”œâ”€â”€ Beats fallidos:  3 (timeout / interval)
â””â”€â”€ Puerto:          5000

Flujo:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     PING      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node A  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Node B  â”‚
â”‚         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     PONG      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Si Node B no responde 3 PINGs consecutivos:
  1. Node A marca Node B como OFFLINE
  2. Node A notifica al Master
  3. Si Node B era Master â†’ se inicia elecciÃ³n Bully
  4. Master coordina re-replicaciÃ³n de archivos de Node B
```

### ElecciÃ³n de LÃ­der (Algoritmo Bully)

| ParÃ¡metro | Valor |
|-----------|-------|
| Trigger | Master no responde a 3 heartbeats consecutivos |
| Criterio | Gana el nodo con mayor `node_id` (lexicogrÃ¡fico) |
| Timeout elecciÃ³n | 5 segundos |
| Timeout coordinador | 10 segundos |
| Tiempo total | ~10-15 segundos |

### Modelo de Consistencia

| Aspecto | ImplementaciÃ³n |
|---------|----------------|
| **Modelo** | Consistencia eventual |
| **Escrituras** | Primero local, luego replicaciÃ³n asÃ­ncrona |
| **Lecturas** | Cualquier rÃ©plica puede responder |
| **Conflictos** | Last-Write-Wins (documentos inmutables) |
| **Convergencia** | Garantizada por inmutabilidad |

### ReplicaciÃ³n y RecuperaciÃ³n

```
Factor de replicaciÃ³n K=2:
â–º Cada documento existe en 2 Slaves
â–º SelecciÃ³n por afinidad semÃ¡ntica

Si Slave-X falla:
1. Master detecta fallo (heartbeat timeout)
2. Master identifica archivos con rÃ©plica Ãºnica
3. Master coordina crear nuevas rÃ©plicas
   â–º Selecciona nuevo destino por afinidad
4. Se restaura factor K=2 en otros Slaves
```

---

## ğŸ“Š MÃ©tricas de Confiabilidad

El sistema registra automÃ¡ticamente mÃ©tricas de confiabilidad:

| MÃ©trica | DescripciÃ³n | FÃ³rmula |
|---------|-------------|---------|
| **MTBF** | Mean Time Between Failures | Tiempo promedio entre fallos |
| **MTTR** | Mean Time To Recovery | Tiempo promedio de recuperaciÃ³n |
| **Disponibilidad** | Uptime del sistema | `MTBF / (MTBF + MTTR)` |

**Endpoint**: `GET /health/cluster`

```json
{
  "cluster_status": "healthy",
  "total_nodes": 3,
  "online_nodes": 3,
  "master_id": "node_1",
  "metrics": {
    "mtbf_hours": 168.5,
    "mttr_seconds": 12.3,
    "availability": 0.9998
  }
}
```

---

## ğŸ”§ ConfiguraciÃ³n por Variables de Entorno

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IDENTIFICACIÃ“N DEL NODO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NODE_ID=node_1                      # ID Ãºnico del nodo (obligatorio)
NODE_ROLE=slave                     # Rol inicial: slave | master
MASTER_CANDIDATE=true               # Â¿Puede ser elegido Master?

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RED
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BACKEND_HOST=0.0.0.0                # IP de escucha del backend
BACKEND_PORT=8000                   # Puerto del API REST
EXTERNAL_IP=172.20.0.11             # IP externa (para otros nodos)
FRONTEND_PORT=8501                  # Puerto de Streamlit

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLUSTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Formato: node_id:ip:api_port:hb_port:election_port, ...
CLUSTER_PEERS=node_2:172.20.0.21:8000:5000:5001,node_3:172.20.0.31:8000:5000:5001
HEARTBEAT_INTERVAL=5                # Segundos entre heartbeats
HEARTBEAT_TIMEOUT=15                # Timeout para considerar nodo caÃ­do
HEARTBEAT_UDP_PORT=5000             # Puerto UDP para heartbeats
ELECTION_UDP_PORT=5001              # Puerto UDP para elecciÃ³n Bully

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REPLICACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REPLICATION_ENABLED=true            # Habilitar replicaciÃ³n
REPLICATION_FACTOR=2                # NÃºmero de rÃ©plicas (K)
CONSISTENCY_MODEL=eventual          # Modelo: eventual | strong
SYNC_INTERVAL_SECONDS=60            # Intervalo de sincronizaciÃ³n

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BASE DE DATOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MONGO_URI=mongodb://localhost:27017 # URI de conexiÃ³n MongoDB
MONGO_DBNAME=distrisearch           # Nombre de la base de datos

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EMBEDDINGS (UbicaciÃ³n SemÃ¡ntica)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EMBEDDING_MODEL=all-MiniLM-L6-v2    # Modelo sentence-transformers

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SEGURIDAD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
JWT_SECRET_KEY=change-me-in-production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
ADMIN_API_KEY=                      # API key para operaciones admin
```

---

## ğŸš€ Despliegue

### Docker Compose - Desarrollo Local

```bash
cd DistriSearch/deploy
docker-compose up -d
```

Levanta:
- 1 Backend (Slave)
- 1 Frontend (Streamlit)
- 1 MongoDB

### Docker Compose - Cluster de Prueba

```bash
cd DistriSearch/deploy
docker-compose -f docker-compose.cluster.yml up -d
```

Levanta:
- 1 CoreDNS
- 3 Nodos completos (Backend + Frontend + MongoDB cada uno)

### URLs de Acceso

| Componente | URL | DescripciÃ³n |
|------------|-----|-------------|
| Frontend Node 1 | http://localhost:8511 | Interfaz Streamlit |
| Frontend Node 2 | http://localhost:8512 | Interfaz Streamlit |
| Frontend Node 3 | http://localhost:8513 | Interfaz Streamlit |
| API Node 1 | http://localhost:8001 | API REST |
| API Node 2 | http://localhost:8002 | API REST |
| API Node 3 | http://localhost:8003 | API REST |
| DNS | distrisearch.local | ResoluciÃ³n Round-Robin |

---

## ğŸ”Œ API Endpoints Completos

### Health Checks

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/health` | GET | Check bÃ¡sico: `{"status": "healthy"}` |
| `/health/detailed` | GET | MÃ©tricas del sistema (CPU, RAM, disco) |
| `/health/cluster` | GET | Estado completo del cluster |
| `/health/ready` | GET | Readiness probe para Kubernetes |
| `/health/live` | GET | Liveness probe para Kubernetes |

### BÃºsqueda

| Endpoint | MÃ©todo | ParÃ¡metros | DescripciÃ³n |
|----------|--------|------------|-------------|
| `/search/?q={query}` | GET | `q`, `file_type`, `max_results` | BÃºsqueda distribuida |
| `/search/stats` | GET | - | EstadÃ­sticas del Ã­ndice |
| `/search/nodes` | GET | - | Lista de nodos disponibles |

### Registro

| Endpoint | MÃ©todo | Body | DescripciÃ³n |
|----------|--------|------|-------------|
| `/register/node` | POST | `NodeInfo` | Registrar nodo en cluster |
| `/register/node/dynamic` | POST | `NodeRegistration` | Auto-registro dinÃ¡mico |
| `/register/files` | POST | `List[FileMeta]` | Registrar metadatos de archivos |
| `/register/upload` | POST | `multipart/form-data` | Subir archivo |
| `/register/heartbeat/{node_id}` | POST | - | Enviar heartbeat |

### Descarga

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/download/{file_id}` | GET | Descargar archivo por ID |
| `/download/info/{file_id}` | GET | InformaciÃ³n del archivo |

### Cluster

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/cluster/status` | GET | Estado actual del cluster |
| `/cluster/nodes` | GET | Lista de todos los nodos |
| `/cluster/master` | GET | Info del Master actual |
| `/cluster/election` | POST | Forzar elecciÃ³n de lÃ­der |

### AutenticaciÃ³n

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/auth/register` | POST | Crear usuario |
| `/auth/login` | POST | Obtener token JWT |
| `/auth/me` | GET | Info del usuario actual |

---

## ğŸ“ FÃ³rmulas y Algoritmos Clave

### Similitud Coseno (BÃºsqueda SemÃ¡ntica)

$$
\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

Donde $A$ es el embedding de la query y $B$ es el perfil del Slave.

### SelecciÃ³n de Nodo para ReplicaciÃ³n

$$
\text{slave\_destino} = \arg\max_{s \in \text{slaves}} \cos(\text{embedding}_{doc}, \text{profile}_s)
$$

### Disponibilidad del Sistema

$$
\text{Disponibilidad} = \frac{\text{MTBF}}{\text{MTBF} + \text{MTTR}}
$$

---

[:octicons-arrow-left-24: Volver](index.md){ .md-button }
[:octicons-arrow-right-24: CaracterÃ­sticas](caracteristicas.md){ .md-button .md-button--primary }
